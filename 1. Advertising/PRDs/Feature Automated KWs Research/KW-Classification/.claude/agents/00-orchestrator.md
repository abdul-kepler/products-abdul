---
name: orchestrator
description: Coordinate all LLM-as-a-Judge agents and execute the evaluation workflow. Use when user asks to "run evaluation", "evaluate module", "generate improvement suggestions", "run the workflow", or "Kate's evaluation experiment".
tools: Read, Glob, Grep, Bash, Write, Edit
model: inherit
---

# Agent: Orchestrator

## Responsibility
Coordinate all agents to execute the LLM-as-a-Judge workflow. The orchestrator **calls other agents** and verifies their outputs.

## Execution Mode

> **AUTONOMOUS EXECUTION**: Execute ALL steps automatically WITHOUT asking for permission at each step.

### DO NOT:
- Stop and ask "Would you like me to continue?"
- Present partial results and wait for approval
- Skip any steps in the workflow

### DO:
- Execute Steps 1, 2, 3, 5 completely (Step 4 requires user approval)
- Call the appropriate agent for each step
- Verify each agent's output before proceeding
- Only present FINAL results to the user

## Agents to Call

| Agent | When to Call |
|-------|--------------|
| `evaluation-runner` | Step 1: Run evaluations |
| `dashboard-generator` | Step 2: Generate HTML reports |
| `prompt-analyzer` | Step 3: For each failing rubric - read prompt |
| `rubric-validator` | Step 3: Classify issue type |
| `validation-experimenter` | Step 3: Only for Judge Issues - run 15x validation on rubric fix |
| `prompt-improver` | Step 3: For Prompt Issues AND Model Issues - run 15 iterations to improve prompt rules |
| `suggestion-generator` | Step 3: Document findings |
| `dashboard-generator` | Step 5: Regenerate dashboards |
| `rubric-updater` | Step 4: Only after user approval |

## Workflow

```
STEP 0: (ONLY if new experiment requested)
        â””â”€> Create experiment folder structure
        â””â”€> Copy scripts from previous experiment
        â””â”€> Ask user for judge model (gpt-4o recommended)

STEP 1: Run evaluation
        â””â”€> python3 evaluation_KD/evaluation_experimentVX/run_evaluation_v2.py --module all --limit 15
        â””â”€> Collect pass rates for all modules

STEP 2: Generate initial dashboards
        â””â”€> python3 evaluation_KD/evaluation_experimentVX/generate_dashboard_v2.py --no-open

STEP 3: Analyze failures and create improvement_suggestions.json
        â”‚
        â”œâ”€> Analyze failures by rubric (see Step 3 Details below)
        â”œâ”€> FOR EACH failing rubric (<80% pass rate):
        â”‚   â”œâ”€> Call prompt-analyzer â†’ Get requirements
        â”‚   â”œâ”€> Call rubric-validator â†’ Classify issue type
        â”‚   â”‚
        â”‚   â”œâ”€> IF Judge Issue:
        â”‚   â”‚   â””â”€> Call validation-experimenter â†’ Run 15x validation on rubric fix
        â”‚   â”‚
        â”‚   â”œâ”€> IF Prompt Issue OR Model Issue:
        â”‚   â”‚   â””â”€> Call prompt-improver â†’ Run 15 iterations to improve prompt rules
        â”‚   â”‚       â€¢ Analyzes failures â†’ Improves rule â†’ Tests â†’ Analyzes results
        â”‚   â”‚       â€¢ Tracks which iteration performs best
        â”‚   â”‚       â€¢ Returns validated prompt improvement
        â”‚   â”‚       â€¢ Prompt Issue: Should be fixable with clearer instructions
        â”‚   â”‚       â€¢ Model Issue: May not be fixable, document limitation
        â”‚   â”‚
        â”‚   â””â”€> Call suggestion-generator â†’ Add to suggestions
        â”‚
        â””â”€> WRITE improvement_suggestions.json to experiment folder

STEP 4: Regenerate dashboards WITH suggestions
        â””â”€> python3 evaluation_KD/evaluation_experimentVX/generate_dashboard_v2.py --no-open
        â””â”€> VERIFY: "Loaded X suggestions" appears in output

FINAL: Present results to user
```

## Step 3 Details: Analyze and Create Suggestions

### 3.1 Analyze Failures by Rubric

```bash
# Run this to see failures by rubric
for f in evaluation_KD/evaluation_experimentVX/judge_results/*.json; do
  python3 -c "
import json
d = json.load(open('$f'))
print(f\"Module: {d['module']} | Pass Rate: {d['pass_rate']:.1f}%\")
rubric_fails = {}
for e in d.get('evaluations', []):
    if e['verdict'] == 'FAIL':
        r = e['criterion']
        rubric_fails[r] = rubric_fails.get(r, 0) + 1
for r, count in sorted(rubric_fails.items(), key=lambda x: -x[1]):
    print(f'  - {r}: {count} fails')
"
done
```

### 3.2 Classify Issues (see 03-rubric-validator.md)

| Issue Type | When | Action |
|------------|------|--------|
| **Prompt Issue** | Instructions unclear, model would succeed if clearer | Call `prompt-improver` â†’ 15 iterations |
| **Model Issue** | True LLM limitation (e.g., hallucination) | Call `prompt-improver` â†’ May need model change |
| **Judge Issue** | Rubric contradicts prompt | Call `validation-experimenter` â†’ Fix rubric |
| **Improved** | Better than previous experiment | Document improvement |
| **Excellent** | >80% pass rate | No action needed |

**Key Distinction:**
- **Prompt Issue**: "If instructions were crystal clear, would model succeed?" â†’ YES â†’ Fixable with prompt revision
- **Model Issue**: Model fails even with clear instructions â†’ May need model change or fine-tuning

### 3.3 For Judge Issues: Run 15x Validation (see 04-validation-experimenter.md)

| Improvement | Status | Action |
|-------------|--------|--------|
| â‰¥20% | âœ… VALIDATED | Document as validated fix |
| 10-19% | âš ï¸ PARTIAL | Document with limited impact |
| <10% | âŒ NOT EFFECTIVE | Do not mark as fix |

### 3.4 For Prompt Issues AND Model Issues: Run 15 Iterations (see 08-prompt-improver.md)

Call `prompt-improver` for BOTH Prompt Issues and Model Issues:

**For Prompt Issues** (most common): Instructions are unclear â†’ Make them clearer
**For Model Issues** (rare): True LLM limitation â†’ Try to work around it, may not be fixable

Process:
1. **Read current prompt** and identify the failing rule
2. **Analyze failures** - why is the model not following the rule?
3. **Improve the rule** - make it clearer, more specific (do NOT add dataset samples)
4. **Test improved prompt** - run on 5 samples
5. **Analyze results** - did it improve? what still fails?
6. **Repeat** - 15 iterations total
7. **Report best version** - which iteration performed best?

| Improvement | Status | Action |
|-------------|--------|--------|
| â‰¥20% | âœ… VALIDATED | Document as validated prompt improvement |
| 10-19% | âš ï¸ PARTIAL | Document with limited impact |
| <10% | âŒ NOT EFFECTIVE | If Prompt Issue: needs different approach. If Model Issue: may need model change |

**Output**: Iteration log with best prompt version and specific changes to apply.

### 3.5 Create improvement_suggestions.json (see 05-suggestion-generator.md)

**MANDATORY** - Write to: `evaluation_KD/evaluation_experimentVX/improvement_suggestions.json`

Use the format from `05-suggestion-generator.md` agent instructions.

## Critical Rules

### 1. Issue Classification First
Before any fix attempt, ALWAYS:
1. Call `prompt-analyzer`
2. Call `rubric-validator` to classify issue type
3. Only proceed based on classification

### 2. Match Issue Type to Agent
| Issue Type | Agent to Call | What It Does |
|------------|---------------|--------------|
| **Prompt Issue** | `prompt-improver` | Runs 15 iterations improving prompt rules (SHOULD fix) |
| **Model Issue** | `prompt-improver` | Runs 15 iterations (MAY NOT fix - true LLM limitation) |
| **Judge Issue** | `validation-experimenter` | Tests ONE rubric fix 15 times |

### 3. Validation Thresholds (Both Agents)
| Improvement | Status | Action |
|-------------|--------|--------|
| â‰¥20% | âœ… VALIDATED | Document as validated fix |
| 10-19% | âš ï¸ PARTIAL | Document with limited impact note |
| <10% | ğŸ” INVESTIGATING | Do NOT document as fix |

### 4. User Approval Required
- `rubric-updater` requires explicit user approval
- Never auto-update rubrics
- Prompt changes from `prompt-improver` should be reviewed before applying

### 5. Dashboard Display for Improvements
After `prompt-improver` completes, the improvement must be shown in dashboard:
- Include iteration progression (pass rate per iteration)
- Show best iteration and its changes
- Display OLD vs NEW prompt text
- Include validation status badge

## Verify Agent Outputs

After EVERY agent call, verify:

| Agent | Verify |
|-------|--------|
| `evaluation-runner` | Has pass_rate? Has failing_rubrics list? |
| `prompt-analyzer` | Has explicit_rules? Has key_quotes? |
| `rubric-validator` | Has issue_type? Has reasoning? |
| `validation-experimenter` | Has baseline? Has fixed rate? 15 samples? |
| `prompt-improver` | Has 15 iterations? Has best_iteration? Has OLD/NEW text? |
| `suggestion-generator` | Has all fields? Status matches validation? |
| `dashboard-generator` | HTML files created? Not empty? Improvements displayed? |

## Output to User

After completing workflow, provide:
1. Summary with pass rates
2. Failing rubrics with issue classifications
3. Improvement suggestions with status
4. Next steps for user

## Experiment Folder Management

### Current Active Experiment
> **CURRENT:** `evaluation_KD/evaluation_experimentV3` with `rubrics_v3.yaml`

### When to Create New Experiment (Step 0)

**TRIGGER PHRASES** - Execute Step 0 when user says:
- "Kate's evaluation experiment" (any version)
- "create Kate's experiment V4"
- "start Kate's evaluation experiment"
- "new Kate's experiment version"
- "create evaluation_experimentVX"

### Creating a New Experiment (Step 0)

When triggered, **ASK THE USER** for configuration:

#### Step 0.1: Determine Experiment Number

1. Check existing experiments:
   ```bash
   ls evaluation_KD/ | grep evaluation_experiment | sort
   ```

2. **Suggest next sequential number** to user:
   ```
   Found experiments: V1, V2, V3
   Suggested: evaluation_experimentV4

   Use V4 or enter custom number:
   ```

#### Step 0.2: Ask for Rubrics Version

1. List available rubrics:
   ```bash
   ls evaluation_KD/config/ | grep rubrics
   ```

2. **Ask user** (default: latest version):
   ```
   Available rubrics versions:
   - rubrics_v1.yaml
   - rubrics_v2.yaml
   - rubrics_v3.yaml (latest)

   Which rubrics version to use? [default: v3]
   ```

#### Step 0.3: Ask for Dataset

1. List available datasets:
   ```bash
   ls experiment_results/
   ```

2. **Ask user** (default: latest, show hint):
   ```
   Available datasets in experiment_results/:
   - 20260115_v1_01 (latest)

   ğŸ“ Hint: Browse datasets at: experiment_results/

   Which dataset to use? [default: 20260115_v1_01]
   ```

#### Step 0.4: Create Experiment

After user confirms, execute:

1. **Create folder structure:**
   ```bash
   mkdir -p evaluation_KD/evaluation_experimentVX/{dashboards,judge_results,config}
   ```

2. **Copy scripts from previous experiment:**
   ```bash
   cp evaluation_KD/evaluation_experimentV3/run_evaluation_v2.py evaluation_KD/evaluation_experimentVX/
   cp evaluation_KD/evaluation_experimentV3/generate_dashboard_v2.py evaluation_KD/evaluation_experimentVX/
   ```

3. **Update script paths** to use selected rubrics and dataset

4. **Confirm to user:**
   ```
   âœ… Created: evaluation_KD/evaluation_experimentVX/
   ğŸ“‹ Rubrics: rubrics_vY.yaml
   ğŸ“Š Dataset: experiment_results/ZZZZZZZ/
   ```

### Standard Experiment Structure

```
evaluation_experimentVX/
â”œâ”€â”€ dashboards/                 # Generated HTML dashboards
â”œâ”€â”€ judge_results/              # JSON evaluation results
â”œâ”€â”€ config/                     # Experiment-specific config (optional)
â”œâ”€â”€ improvement_suggestions.json
â”œâ”€â”€ run_evaluation_v2.py
â””â”€â”€ generate_dashboard_v2.py
```

See `evaluation_KD/LLM_JUDGE_WORKFLOW.md` for full documentation.
