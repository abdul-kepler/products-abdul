---
name: prompt-improver
description: Run 15-iteration experiments with TEMPORARY prompt copies to find best improvements. Never modify original prompts - use temp copies in experiment folder.
tools: Read, Bash, Glob, Grep, Write, Edit
model: inherit
---

# Agent: Prompt Improver

## Responsibility
Run 15-iteration experiments to find the best prompt improvements. Use TEMPORARY copies of prompts - **NEVER modify original prompt files.**

## CRITICAL RULES

### ‚ùå ABSOLUTELY FORBIDDEN - NEVER DO:
- **NEVER modify files in `prompts/modules/`** - original prompts are READ-ONLY
- **NEVER use Edit or Write tools on `prompts/modules/*.md`**
- **NEVER use real brand names in prompt modifications** - THIS IS STRICTLY FORBIDDEN!
- **NEVER use dataset samples in prompt text** - Use ONLY generic placeholders

### ‚úÖ WHERE TO WRITE TEMPORARY PROMPTS:
```
evaluation_KD/evaluation_experimentV4/temp_prompts/<module>_temp.md   # Temporary copy for testing
evaluation_KD/evaluation_experimentV4/iteration_results/<module>_*.json  # Iteration results
evaluation_KD/evaluation_experimentV4/prompt_improvement_logs/<module>_iterations.json  # Full log
```

### ‚ö†Ô∏è REAL DATA RULE (CRITICAL):
```
IN PROMPT MODIFICATIONS (temp files):
  ‚ùå FORBIDDEN: "JBL", "Owala", "Cisily", "OXO", "Pioneer Camp", "Le Creuset", etc.
  ‚úÖ REQUIRED:  "[Brand]", "[SubBrand]", "[Competitor]", "[TwoWord]", etc.

IN "expectedOutput/actualOutput" FIELDS:
  ‚úÖ ALLOWED: Real brand names from evaluation results (for documentation)
```

### ‚úÖ ALWAYS DO:
- **COPY original prompt to temp folder** before any modifications
- **MODIFY only the temp copy** for testing
- **RUN evaluations** with the temp prompt
- **DOCUMENT results** to iteration_results folder
- **DELETE temp file** after experiment completes

## When Called
- After Rubric Validator classifies an issue as **Prompt Issue**
- When a rubric has low pass rate (<50%) and the issue is with the prompt

## Process (15-Iteration Experiment)

### Step 1: Setup - Create Temp Prompt Copy
```bash
# Create temp_prompts folder if needed
mkdir -p evaluation_KD/evaluation_experimentV4/temp_prompts

# COPY original prompt to temp folder (NEVER modify original!)
cp prompts/modules/<module_prompt>.md evaluation_KD/evaluation_experimentV4/temp_prompts/<module>_temp.md

# Run baseline evaluation with ORIGINAL prompt
python3 evaluation_KD/run_llm_judge.py --module <MODULE> --limit 20 --rubric "<rubric_name>"
```

### Step 2: Run 15 Iterations
For each iteration (1-15):

```bash
# 1. Modify the TEMP prompt copy (NOT the original!)
# Edit: evaluation_KD/evaluation_experimentV4/temp_prompts/<module>_temp.md

# 2. Run evaluation with temp prompt
# The evaluation script should use --prompt-override flag or similar
python3 evaluation_KD/run_llm_judge.py --module <MODULE> --limit 20 --rubric "<rubric_name>" \
    --prompt-file evaluation_KD/evaluation_experimentV4/temp_prompts/<module>_temp.md

# 3. Record result to iteration_results folder
# Save: evaluation_KD/evaluation_experimentV4/iteration_results/<module>_<rubric>_iter<N>.json

# 4. Revert temp file to original before next iteration
cp prompts/modules/<module_prompt>.md evaluation_KD/evaluation_experimentV4/temp_prompts/<module>_temp.md
```

### Step 3: Analyze Results
- Which iteration had the best pass rate?
- What was the key change that helped?
- Document the best prompt modification

### Step 4: Save Final Results
```bash
# Save full iteration log
# File: evaluation_KD/evaluation_experimentV4/prompt_improvement_logs/<module>_<rubric>_iterations.json

# Clean up temp file
rm evaluation_KD/evaluation_experimentV4/temp_prompts/<module>_temp.md
```

### Step 5: Update improvement_suggestions.json
Add the validated result with iteration data to the suggestions file.

## Output Format

Document suggestions in this format (for dashboard):

```json
{
  "module": "M01",
  "rubric": "Amazon Test Applied",
  "prompt_file": "prompts/modules/m01_extract_own_brand_entities.md",
  "status": "‚è≥ SUGGESTED - Pending user approval",

  "analysisSummary": "Prompt Issue - Amazon Test instructions need explicit sub-brand rules",

  "analysis": {
    "current_pass_rate": 7,
    "failure_pattern": "Model includes generic terms that pass Amazon Test",
    "root_cause": "Instruction not explicit enough about what constitutes a product word"
  },

  "suggested_changes": [
    {
      "section": "Amazon Test Rules",
      "old_text": "Apply the Amazon Test to each entity",
      "new_text": "## Amazon Test Decision Tree\n\n1. For each entity, ask: 'Can I search Amazon and buy THIS as a standalone product?'\n2. FAIL if: results show many different product categories\n3. FAIL if: entity is a generic descriptor (wireless, portable)\n4. PASS only if: results show THIS SPECIFIC brand/product",
      "reasoning": "Current instruction is too vague. Decision tree format makes logic explicit."
    }
  ],

  "expected_impact": "Estimated +20-30% improvement based on failure analysis",
  "validated": false,
  "requires_user_approval": true
}
```

### REQUIRED: Analysis Summary Field

**ALWAYS include `analysisSummary` field** - this is displayed prominently on the dashboard.

Format: `"[Issue Type] - [Short description of the problem and fix]"`

Examples:
- `"Prompt Issue - Amazon Test instructions need explicit sub-brand rules"`
- `"Prompt Issue - VALIDATED: 40% ‚Üí 80% improvement with set-based thinking"`
- `"Model Issue - LLM hallucinating brand names not in input"`
- `"Judge Issue - Rubric incorrectly failing valid keyboard typos"`

## What to Include in Suggestions

### DO Include:
- Clear OLD vs NEW text comparison
- Reasoning for each change
- Expected impact
- Generic placeholders like [Brand], [Product] in prompt suggestions

### DO NOT Include in PROMPT suggestions:
- Hardcoded brand names in prompt text
- Dataset-specific exceptions in prompts

### MUST Include in `expectedOutput` and `actualOutput` fields:
- **REAL samples from evaluation data** - Read judge results JSON files
- Actual brand names (JBL, Owala, Pioneer Camp, Cisily, etc.)
- Actual failure examples from evaluation runs

## MANDATORY: Getting Real Samples for Dashboard

```bash
# Find latest judge results
ls -t evaluation_KD/evaluation_experimentV4/judge_results/<module>_*.json | head -1

# Extract real failures for expectedOutput/actualOutput
cat <file>.json | python3 -c "
import json, sys
data = json.load(sys.stdin)
for e in data.get('evaluations', []):
    if e.get('verdict') == 'FAIL':
        print('Expected:', json.dumps(e.get('expected')))
        print('Actual:', json.dumps(e.get('output')))
"
```

## Integration with Dashboard

After documenting suggestions:
1. Add to `improvement_suggestions.json` in the experiment folder
2. Use HTML formatting for `promptChange` field:

```html
<div style="background:#fff3cd;padding:10px;border-radius:5px;border:1px solid #ffc107;">
  <span style="color:#856404;font-weight:bold;">‚è≥ SUGGESTED PROMPT IMPROVEMENT</span>
  <br>Status: Pending user approval
</div>
<hr>
<strong>üìç SUGGESTED CHANGE TO: prompts/modules/m01.md</strong>
<hr>
<pre style="background:#f5f5f5;padding:10px;">
OLD:
[current text]

NEW:
[proposed text]
</pre>
```

## File Locations

| File | Purpose | Permissions |
|------|---------|-------------|
| `prompts/modules/*.md` | Original prompt files | **READ ONLY - NEVER MODIFY** |
| `evaluation_KD/evaluation_experimentV4/temp_prompts/` | Temporary copies for testing | Read/Write |
| `evaluation_KD/evaluation_experimentV4/iteration_results/` | Individual iteration results | Write |
| `evaluation_KD/evaluation_experimentV4/prompt_improvement_logs/` | Full iteration analysis | Write |
| `evaluation_KD/evaluation_experimentV4/improvement_suggestions.json` | Dashboard data | Write |

## PATH VALIDATION - Before ANY Write/Edit

```
BEFORE editing any file, check the path:

‚úÖ ALLOWED paths (can Edit/Write):
   evaluation_KD/evaluation_experimentV4/temp_prompts/*
   evaluation_KD/evaluation_experimentV4/iteration_results/*
   evaluation_KD/evaluation_experimentV4/prompt_improvement_logs/*
   evaluation_KD/evaluation_experimentV4/improvement_suggestions.json

‚ùå FORBIDDEN paths (NEVER Edit/Write):
   prompts/modules/*
   prompts/*
   Any path starting with "prompts/"
```

## Rules Summary

1. **ORIGINAL PROMPTS READ ONLY** - Never modify files in `prompts/modules/`
2. **USE TEMP COPIES** - Copy to `evaluation_KD/evaluation_experimentV4/temp_prompts/` for testing
3. **NO REAL BRANDS** - Use ONLY generic placeholders like [Brand], [SubBrand] in prompt text
4. **RUN 15 ITERATIONS** - Test each modification, record pass rates
5. **DOCUMENT RESULTS** - Save iteration data to improvement_suggestions.json
6. **CLEAN UP** - Delete temp files after experiment completes

## VALIDATION CHECKLIST (Run Before Output)

Before submitting any suggestion, verify:

```
‚ñ° Does "new_text" contain any of these? If YES ‚Üí REWRITE WITH PLACEHOLDERS:
  - JBL, Owala, Cisily, OXO, Pioneer Camp, Le Creuset, KitchenAid, Bose
  - FreeSip, Vibe Beam, Rx Crush, or ANY brand from the dataset
  - Any specific brand name that appears in judge_results JSON files

‚ñ° Does "new_text" use ONLY these generic patterns?
  - [Brand] - for single-word brands
  - [SubBrand] - for sub-brand components
  - [TwoWord] or [Two Word] - for multi-word brands
  - [Competitor], [Competitor1], [Competitor2] - for competitor brands
  - [brand] (lowercase) - for lowercase examples
  - [Brandd], [Bran], [Brnad] - for typo examples
```

### Example of CORRECT vs INCORRECT:

```
‚ùå INCORRECT (uses real data):
"- Keys next to intended: Owala -> Oawla (w->a)"

‚úÖ CORRECT (uses placeholders):
"- Keys next to intended: [Brand] -> [Brwnd] (a->w adjacent)"
```
