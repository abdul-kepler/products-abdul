# Agent: Orchestrator (Claude)

## Responsibility
Coordinate all agents to execute the LLM-as-a-Judge workflow. The orchestrator reads user requests, delegates to appropriate agents, and ensures the workflow is followed correctly.

## Execution Mode

> **AUTONOMOUS EXECUTION REQUIRED**: When asked to "run the evaluation workflow" or "generate improvement suggestions", Claude MUST execute ALL steps in this framework automatically WITHOUT asking for permission at each step.

### DO NOT:
- Stop and ask "Would you like me to continue?"
- Ask "Should I run the validation experiments?"
- Present partial results and wait for approval
- Skip any steps in the workflow

### DO:
- Read this orchestrator document first (reference LLM_JUDGE_WORKFLOW.md for details)
- Execute Steps 1, 2, 3, 5 completely (Step 4 requires user approval)
- For each failing rubric: classify issue type FIRST, then decide action
- Run 15x validation experiments ONLY for Judge Issues (not Model/Prompt Issues)
- Only present FINAL validated results to the user
- Mark suggestions as âœ… VALIDATED only after completing 15x validation with â‰¥20% improvement

> **Exception**: Step 4 (Update Rubrics) requires explicit user approval before modifying rubric files.

## Available Agents

| Agent | File | Responsibility |
|-------|------|----------------|
| **Evaluation Runner** | `01_evaluation_runner.md` | Run evaluations, collect pass/fail results |
| **Prompt Analyzer** | `02_prompt_analyzer.md` | Read prompts, extract requirements |
| **Rubric Validator** | `03_rubric_validator.md` | Compare rubrics vs prompts, classify issues |
| **Validation Experimenter** | `04_validation_experimenter.md` | Run 15x validation experiments |
| **Suggestion Generator** | `05_suggestion_generator.md` | Create improvement suggestions |
| **Dashboard Generator** | `06_dashboard_generator.md` | Generate HTML dashboards |
| **Rubric Updater** | `07_rubric_updater.md` | Update rubrics (with user approval) |

## Workflow Steps Mapping

| Step | Description | Agent(s) |
|------|-------------|----------|
| Step 1 | Run Evaluation Script | Evaluation Runner |
| Step 2 | Generate HTML Report | Dashboard Generator (judge reports) |
| Step 3 | Analyze Results & Generate Suggestions | Prompt Analyzer â†’ Rubric Validator â†’ (Validation Experimenter for Judge Issues only) â†’ Suggestion Generator |
| Step 4 | Update Rubrics (After User Approval) | Rubric Updater |
| Step 5 | Regenerate Dashboards with Suggestions | Dashboard Generator (dashboards) |

## Full Evaluation Workflow

When user says "run evaluation workflow" or similar:

```
STEP 1: [Evaluation Runner] Run evaluation on requested modules
        â””â”€> Collect pass/fail results
        â””â”€> Identify failing rubrics (<50% pass rate)

STEP 2: [Dashboard Generator] Generate HTML report for each module
        â””â”€> python3 evaluation/generate_judge_report.py <MODULE>
        â””â”€> (This creates judge_report_<module>.html for detailed analysis)

STEP 3: FOR EACH failing rubric (<50% pass rate):
        â”‚
        â”œâ”€> [Prompt Analyzer] Read the module's prompt file
        â”‚   â””â”€> Extract requirements, constraints, expected behaviors
        â”‚
        â”œâ”€> [Rubric Validator] Compare rubric vs prompt
        â”‚   â””â”€> Classify issue type (Model/Judge/Prompt Issue)
        â”‚
        â”œâ”€> IF issue type is JUDGE ISSUE:
        â”‚   â”‚   (Rubric contradicts prompt)
        â”‚   â”‚
        â”‚   â”œâ”€> [Validation Experimenter] Run 15x validation
        â”‚   â”‚   â””â”€> Test proposed rubric fix on 15 samples
        â”‚   â”‚   â””â”€> Calculate improvement percentage
        â”‚   â”‚
        â”‚   â”œâ”€> IF improvement â‰¥20%:
        â”‚   â”‚   â””â”€> [Suggestion Generator] Document as âœ… VALIDATED
        â”‚   â”‚   â””â”€> (Writes to improvement_suggestions.json)
        â”‚   â”‚
        â”‚   â”œâ”€> IF improvement 10-19%:
        â”‚   â”‚   â””â”€> [Suggestion Generator] Document as âš ï¸ PARTIAL
        â”‚   â”‚   â””â”€> (Writes to improvement_suggestions.json)
        â”‚   â”‚
        â”‚   â””â”€> IF improvement <10%:
        â”‚       â””â”€> Do NOT document as fix
        â”‚       â””â”€> Status: ğŸ” INVESTIGATING
        â”‚       â””â”€> Investigate further why fix didn't work
        â”‚
        â”œâ”€> IF issue type is PROMPT ISSUE:
        â”‚   â”‚   (Prompt is unclear/ambiguous)
        â”‚   â”‚
        â”‚   â””â”€> [Suggestion Generator] Document as Prompt Issue
        â”‚       â””â”€> Recommend prompt clarification FIRST
        â”‚       â””â”€> Do NOT run validation until prompt is clear
        â”‚       â””â”€> Status: ğŸ” INVESTIGATING
        â”‚       â””â”€> (Writes to improvement_suggestions.json)
        â”‚
        â””â”€> IF issue type is MODEL ISSUE:
            â”‚   (Rubric matches prompt, model doesn't follow)
            â”‚
            â””â”€> [Suggestion Generator] Document as Model Issue
                â””â”€> Do NOT recommend rubric change
                â””â”€> Recommend investigating model behavior
                â””â”€> Status: ğŸ” INVESTIGATING
                â””â”€> (Writes to improvement_suggestions.json)

(STEP 4 is skipped in automatic flow - requires user approval)

STEP 5: [Dashboard Generator] Regenerate dashboards with suggestions
        â””â”€> python3 evaluation_experimentV2/generate_dashboard_v2.py --no-open
        â””â”€> Reads from improvement_suggestions.json (written by Suggestion Generator in Step 3)
        â””â”€> Updates MODULE_ANALYSIS_DASHBOARD.html with latest suggestions

FINAL: Present FINAL results to user
```

## Critical Rules

### 1. Rubrics Must Match Prompts

> **NEVER change a rubric to "fix" a failing evaluation unless the rubric contradicts the prompt.**

Before any rubric modification:
1. ALWAYS call Prompt Analyzer first
2. ALWAYS call Rubric Validator to classify issue type
3. If rubric matches prompt â†’ Model Issue â†’ Do NOT change rubric

| Scenario | Root Cause | Action |
|----------|------------|--------|
| Rubric says X, Prompt says X, Model does Y | Model Issue | Investigate model/prompt, NOT rubric |
| Rubric says X, Prompt says Y | Judge Issue | Fix rubric to match prompt |
| Rubric says X, Prompt unclear | Prompt Issue | Clarify prompt first |

**WRONG:** "Rubric is too strict, let me make it flexible"
**RIGHT:** "Does the rubric match the prompt? If yes, why isn't model following the prompt?"

### 2. 15x Validation Required

> **No suggestion can be marked as VALIDATED without running 15x experiments.**

- Minimum 15 samples tested
- Must show â‰¥20% improvement to be VALIDATED
- Record baseline and fixed pass rates

#### Validation Thresholds

| Improvement | Status | Action |
|-------------|--------|--------|
| â‰¥20% improvement | âœ… VALIDATED | Document as validated fix |
| 10-19% improvement | âš ï¸ PARTIAL | Document but note limited impact |
| <10% improvement | âŒ NOT EFFECTIVE | Do NOT document, investigate further |
| Inconsistent results | ğŸ” INVESTIGATING | Run more samples, check for edge cases |

### 3. User Approval for Rubric Changes

> **Rubric Updater MUST NOT execute without explicit user approval.**

- Always present findings first
- Wait for user to say "yes" or approve
- Never auto-update rubrics

## Status Labels

Use these status indicators consistently:

| Status | Label | When to Use |
|--------|-------|-------------|
| âœ… VALIDATED | `validated: true` | Fix tested and confirmed (â‰¥20% improvement) |
| â³ PENDING | `validated: false` | Proposed but not yet tested |
| âš ï¸ PARTIAL | `validated: true` | Fix helps but doesn't fully solve (10-19%) |
| ğŸ” INVESTIGATING | `validated: false` | Root cause still being analyzed |

## Rubric Update Workflow (Requires Approval)

When user requests a rubric update (e.g., "update rubric" or "apply the fix"):

```
1. [Rubric Updater] Present change details to user
   â””â”€> Show OLD vs NEW, validation results

2. WAIT for user approval
   â””â”€> User must explicitly say "yes" or approve

3. IF approved:
   â””â”€> [Rubric Updater] Apply change to rubrics_v2.yaml
   â””â”€> [Dashboard Generator] Regenerate HTML

4. IF not approved:
   â””â”€> Document as pending, do not change
```

## Agent Calling Protocol

### Correct Order
```
User Request â†’ Orchestrator â†’ Agent â†’ VERIFY OUTPUT â†’ Next Agent â†’ VERIFY â†’ ... â†’ Final Output
```

### CRITICAL: Verify Every Agent Output

> **After EVERY agent call, the Orchestrator MUST verify the output before proceeding.**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Call Agent     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Get Result     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  VERIFY: Did agent do its job?  â”‚
â”‚  - Is output in expected format?â”‚
â”‚  - Are all required fields present?â”‚
â”‚  - Does result make sense?      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â”‚         â”‚
    â–¼         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  OK   â”‚  â”‚  FAILED   â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
    â”‚            â”‚
    â–¼            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Continueâ”‚  â”‚ Retry or Report Errorâ”‚
â”‚ to next â”‚  â”‚ Do NOT proceed with  â”‚
â”‚ agent   â”‚  â”‚ bad data             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Verification Checklist Per Agent

| Agent | Expected Output | Verify |
|-------|-----------------|--------|
| **Evaluation Runner** | JSON with pass/fail counts | âœ“ File exists? âœ“ Has pass_rate? âœ“ Has failing_rubrics list? |
| **Prompt Analyzer** | Extracted requirements | âœ“ Has explicit_rules? âœ“ Has key_quotes from prompt? âœ“ Count constraints found? |
| **Rubric Validator** | Issue classification | âœ“ Has issue_type (Model/Judge/Prompt)? âœ“ Has reasoning? âœ“ Classification makes sense? |
| **Validation Experimenter** | Before/after comparison | âœ“ Has baseline pass rate? âœ“ Has fixed pass rate? âœ“ Improvement calculated? âœ“ 15 samples tested? |
| **Suggestion Generator** | Suggestion JSON | âœ“ Has all required fields? âœ“ Status matches validation results? âœ“ issueType correct? |
| **Dashboard Generator** | HTML files | âœ“ Files created? âœ“ Data populated (not empty)? |
| **Rubric Updater** | Updated YAML | âœ“ Change applied correctly? âœ“ Old value backed up? âœ“ User approved? |

### When to Call Each Agent

| Trigger | Agent to Call |
|---------|---------------|
| "Run evaluation" | Evaluation Runner |
| After evaluation completes | Dashboard Generator (generate reports) |
| Failing rubric found (<50%) | Prompt Analyzer â†’ Rubric Validator |
| Judge Issue identified | Validation Experimenter â†’ Suggestion Generator |
| Prompt Issue identified | Suggestion Generator (recommend prompt clarification, NO validation) |
| Model Issue identified | Suggestion Generator (document as model issue, NO rubric change) |
| "Update rubric" or user approves | Rubric Updater |
| "Generate dashboard" | Dashboard Generator |
| After suggestions documented | Dashboard Generator (regenerate with suggestions) |

## Output to User

After completing workflow, provide:

1. **Summary Table**: All modules with pass rates
2. **Failing Rubrics**: List with issue classifications (Model/Judge/Prompt)
3. **Improvement Suggestions**: With status (âœ…/â³/âš ï¸/ğŸ”)
4. **Actions Taken**: What was updated (if anything)
5. **Next Steps**: What requires user decision

## Example Orchestration (with Verification)

```
User: "Run evaluation on M01 and generate improvement suggestions"

Orchestrator:
â”‚
â”œâ”€> STEP 1: [Evaluation Runner]
â”‚   â”‚   Command: python3 evaluation_experimentV2/run_evaluation_v2.py --module m01 --limit 15
â”‚   â”‚
â”‚   â”‚   VERIFY OUTPUT:
â”‚   â”‚   âœ“ File created: m01_judge_20260116_120000.json
â”‚   â”‚   âœ“ Has pass_rate: 34.7%
â”‚   â”‚   âœ“ Has failing_rubrics: 2 rubrics with <50% pass rate
â”‚   â”‚   âœ“ OUTPUT VALID â†’ Continue
â”‚   â”‚
â”‚   Result: 34.7% pass rate, 2 failing rubrics
â”‚
â”œâ”€> STEP 2: [Dashboard Generator]
â”‚   â”‚   Command: python3 evaluation/generate_judge_report.py m01
â”‚   â”‚
â”‚   â”‚   VERIFY OUTPUT:
â”‚   â”‚   âœ“ File created: evaluation/judge_report_m01.html
â”‚   â”‚   âœ“ File size > 0 (not empty)
â”‚   â”‚   âœ“ Contains evaluation data for M01
â”‚   â”‚   âœ“ OUTPUT VALID â†’ Continue
â”‚   â”‚
â”‚   Result: judge_report_m01.html generated for detailed analysis
â”‚
â”œâ”€> STEP 3: FOR rubric "No Duplicate Entities" (33% pass rate):
â”‚   â”‚
â”‚   â”œâ”€> [Prompt Analyzer] Read m01_extract_own_brand_entities.md
â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   VERIFY OUTPUT:
â”‚   â”‚   â”‚   âœ“ Has explicit_rules: ["Case variation | All lowercase..."]
â”‚   â”‚   â”‚   âœ“ Has key_quotes from prompt
â”‚   â”‚   â”‚   âœ“ OUTPUT VALID â†’ Continue
â”‚   â”‚   â”‚
â”‚   â”‚   Result: Prompt allows case variations as valid entities
â”‚   â”‚
â”‚   â”œâ”€> [Rubric Validator] Compare rubric vs prompt
â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   VERIFY OUTPUT:
â”‚   â”‚   â”‚   âœ“ Has issue_type: "Judge Issue"
â”‚   â”‚   â”‚   âœ“ Has reasoning: "Rubric marks case variations as duplicates..."
â”‚   â”‚   â”‚   âœ“ Rubric says: "no duplicates" vs Prompt says: "case variations allowed"
â”‚   â”‚   â”‚   âœ“ Classification MAKES SENSE â†’ Continue
â”‚   â”‚   â”‚
â”‚   â”‚   Result: "Judge Issue" - Rubric contradicts prompt
â”‚   â”‚
â”‚   â”œâ”€> [Validation Experimenter] Test fix on 15 samples
â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   VERIFY OUTPUT:
â”‚   â”‚   â”‚   âœ“ samples_tested: 15 (not less!)
â”‚   â”‚   â”‚   âœ“ baseline: 33%
â”‚   â”‚   â”‚   âœ“ with_fix: 80%
â”‚   â”‚   â”‚   âœ“ improvement: 47% (â‰¥20% threshold)
â”‚   â”‚   â”‚   âœ“ OUTPUT VALID â†’ Continue
â”‚   â”‚   â”‚
â”‚   â”‚   Result: 33% â†’ 80% (+47% improvement) âœ…
â”‚   â”‚
â”‚   â””â”€> [Suggestion Generator] Create suggestion
â”‚       â”‚
â”‚       â”‚   VERIFY OUTPUT:
â”‚       â”‚   âœ“ Has module: "M01"
â”‚       â”‚   âœ“ Has issueType: "Judge Issue"
â”‚       â”‚   âœ“ Has validated: true (matches â‰¥20% improvement)
â”‚       â”‚   âœ“ Has promptChange with OLD vs NEW
â”‚       â”‚   âœ“ OUTPUT VALID â†’ Continue
â”‚       â”‚
â”‚       Result: Documented as âœ… VALIDATED
â”‚
â”œâ”€> STEP 3: FOR rubric "8-12 Variations" (27% pass rate):
â”‚   â”‚
â”‚   â”œâ”€> [Prompt Analyzer] Read prompt
â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   VERIFY OUTPUT:
â”‚   â”‚   â”‚   âœ“ Has count_constraint: "EXACTLY 8-12 variations"
â”‚   â”‚   â”‚   âœ“ Has key_quotes: ["EXACTLY 8-12 variations â€” no more, no less"]
â”‚   â”‚   â”‚   âœ“ OUTPUT VALID â†’ Continue
â”‚   â”‚   â”‚
â”‚   â”‚   Result: Prompt says "EXACTLY 8-12 variations"
â”‚   â”‚
â”‚   â”œâ”€> [Rubric Validator] Compare rubric vs prompt
â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   VERIFY OUTPUT:
â”‚   â”‚   â”‚   âœ“ Has issue_type: "Model Issue"
â”‚   â”‚   â”‚   âœ“ Has reasoning: "Rubric correctly requires 8-12..."
â”‚   â”‚   â”‚   âœ“ Rubric says: "8-12" vs Prompt says: "EXACTLY 8-12"
â”‚   â”‚   â”‚   âœ“ MATCH! â†’ Classification "Model Issue" MAKES SENSE
â”‚   â”‚   â”‚   âœ“ OUTPUT VALID â†’ Continue (NO validation needed)
â”‚   â”‚   â”‚
â”‚   â”‚   Result: "Model Issue" - Rubric matches prompt, model not following
â”‚   â”‚
â”‚   â””â”€> [Suggestion Generator] Document as Model Issue
â”‚       â”‚
â”‚       â”‚   VERIFY OUTPUT:
â”‚       â”‚   âœ“ Has issueType: "Model Issue"
â”‚       â”‚   âœ“ Has validated: false (no validation run for Model Issues)
â”‚       â”‚   âœ“ Does NOT recommend rubric change
â”‚       â”‚   âœ“ OUTPUT VALID â†’ Continue
â”‚       â”‚
â”‚       Result: Documented as ğŸ” INVESTIGATING (Model Issue)
â”‚       NOTE: No validation run - this is a Model Issue, not Judge Issue
â”‚
â”‚   (STEP 4 skipped - requires user approval to update rubrics)
â”‚
â”œâ”€> STEP 5: [Dashboard Generator] Regenerate dashboards
â”‚   â”‚   Command: python3 evaluation_experimentV2/generate_dashboard_v2.py --no-open
â”‚   â”‚
â”‚   â”‚   VERIFY OUTPUT:
â”‚   â”‚   âœ“ MODULE_ANALYSIS_DASHBOARD.html updated
â”‚   â”‚   âœ“ improvement_suggestions.json updated
â”‚   â”‚   âœ“ Contains 2 suggestions from this run
â”‚   â”‚   âœ“ OUTPUT VALID â†’ Continue
â”‚   â”‚
â”‚   Result: Dashboards regenerated
â”‚
â””â”€> Present FINAL results to user:

    EVALUATION RESULTS: M01
    ============================================================
    Overall Pass Rate: 34.7%

    FAILING RUBRICS:
    | Rubric | Pass Rate | Issue Type | Status |
    |--------|-----------|------------|--------|
    | No Duplicate Entities | 33% | Judge Issue | âœ… VALIDATED |
    | 8-12 Variations | 27% | Model Issue | ğŸ” INVESTIGATING |

    NEXT STEPS:
    - "No Duplicate Entities" fix ready. Say "update rubric" to apply.
    - "8-12 Variations" is a model issue - rubric is correct.
```

### Example: Handling Verification Failure

```
â”œâ”€> [Validation Experimenter] Test fix on 15 samples
â”‚   â”‚
â”‚   â”‚   VERIFY OUTPUT:
â”‚   â”‚   âœ“ samples_tested: 15
â”‚   â”‚   âœ“ baseline: 33%
â”‚   â”‚   âœ“ with_fix: 38%
â”‚   â”‚   âœ— improvement: 5% (BELOW 20% threshold!)
â”‚   â”‚   âš ï¸ FIX NOT EFFECTIVE
â”‚   â”‚
â”‚   â”‚   ACTION: Do NOT proceed to Suggestion Generator with "VALIDATED"
â”‚   â”‚   â†’ Document as ğŸ” INVESTIGATING instead
â”‚   â”‚   â†’ Report: "Proposed fix only improved 5%. Need different approach."
â”‚   â”‚
â”‚   Result: Fix NOT effective - investigating further
```

## File Locations

| Purpose | Path |
|---------|------|
| Agent definitions | `evaluation_KD/agents/*.md` |
| Main workflow | `evaluation_KD/LLM_JUDGE_WORKFLOW.md` |
| Rubric config | `evaluation_KD/config/rubrics_v2.yaml` |
| Judge results | `evaluation_experimentV2/judge_results/` |
| Improvement suggestions | `evaluation_experimentV2/improvement_suggestions.json` |
| Dashboards | `evaluation_experimentV2/dashboards/` |
| Prompts | `prompts/modules/*.md` |
