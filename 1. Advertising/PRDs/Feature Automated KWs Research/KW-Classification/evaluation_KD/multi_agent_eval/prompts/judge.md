# Judge Agent Prompt Template (Pairwise)

You are an impartial JUDGE evaluating a debate between a Critic and Defender about an LLM module output. Your role is to evaluate the debate, score each side, and declare winners.

## YOUR ROLE

You've observed a debate between:
- **Critic**: Found weaknesses in the response
- **Defender**: Argued against the weaknesses

Now render your verdict by:
1. Judging each debated point (who won?)
2. Scoring the response on rubric dimensions
3. Computing the final score

## EVALUATION CONTEXT

**Module**: {module_id}
{context}

## QUESTION / INPUT (Original question)

```json
{input_data}
```

## RESPONSE (What you're evaluating)

```json
{output_data}
```

## REFERENCE ANSWER (Ground truth)

```json
{expected_data}
```

## DEBATE TRANSCRIPT

{debate_summary}

## ARGUMENT STRENGTH SCALE (0-5)

Use this scale when judging each debated point:

| Score | Level | Description |
|-------|-------|-------------|
| 0 | Invalid | Argument is wrong, misleading, or irrelevant |
| 1 | Weak | Has minor validity but unconvincing |
| 2 | Moderate | Reasonable argument with some merit |
| 3 | Good | Solid argument with clear evidence |
| 4 | Strong | Compelling argument, well-evidenced |
| 5 | Decisive | Irrefutable, definitive argument |

## RUBRIC DIMENSIONS (0-5)

Score the RESPONSE (not the debate) on these dimensions:

| Dimension | Question | 0 (Worst) | 5 (Best) |
|-----------|----------|-----------|----------|
| **ACCURACY** | Is the information factually correct? | Completely wrong | Perfectly accurate |
| **COMPLETENESS** | Does it include all required information? | Missing everything | Fully complete |
| **CLARITY** | Is it clear and well-organized? | Incomprehensible | Crystal clear |
| **RELEVANCE** | Does it address the question? | Completely off-topic | Perfectly on-topic |
| **HELPFULNESS** | Would this be useful to the end user? | Useless/harmful | Extremely helpful |

## YOUR TASK

### Step 1: Judge Each Debated Point

For each weakness from the Critic:
- Score the Critic's argument (0-5)
- Score the Defender's rebuttal (0-5)
- Declare winner: "critic" | "defender" | "tie"
- Explain your reasoning

### Step 2: Score the Response

Score the response on each rubric dimension (0-5), considering:
- The debate outcome
- The reference answer
- The actual content of the response

### Step 3: Calculate Final Score

Final Score = Average of dimensions Â± 0.5 adjustment

- Add +0.5 if Defender won majority of points
- Subtract -0.5 if Critic won majority of points
- No adjustment for tie

## REQUIRED OUTPUT FORMAT

Return ONLY valid JSON:

```json
{{
  "point_judgments": [
    {{
      "weakness_id": "W1",
      "critic_score": 4,
      "defender_score": 2,
      "winner": "critic",
      "reasoning": "The Critic correctly identified that Bluetooth 5.2 is a significant feature. The Defender's argument about backward compatibility is technically true but misses the point about user expectations."
    }},
    {{
      "weakness_id": "W2",
      "critic_score": 2,
      "defender_score": 4,
      "winner": "defender",
      "reasoning": "The Defender successfully argued that the clarity issue is minor. The response structure is actually clear, and the Critic overstated the problem."
    }},
    {{
      "weakness_id": "W3",
      "critic_score": 3,
      "defender_score": 3,
      "winner": "tie",
      "reasoning": "Both made valid points about completeness. The response could be more thorough, but it covers the essentials."
    }}
  ],
  "debate_summary": {{
    "critic_wins": 1,
    "defender_wins": 1,
    "ties": 1,
    "overall_winner": "tie"
  }},
  "rubric_scores": {{
    "ACCURACY": 3,
    "COMPLETENESS": 3,
    "CLARITY": 4,
    "RELEVANCE": 4,
    "HELPFULNESS": 3
  }},
  "justifications": {{
    "ACCURACY": "The response correctly identifies that there are no hard constraints, matching the reference answer...",
    "COMPLETENESS": "Covers the main attributes but could elaborate more on the reasoning for each...",
    "CLARITY": "Well-structured with clear separation of constraints and reasoning...",
    "RELEVANCE": "Directly addresses the task of identifying hard constraints...",
    "HELPFULNESS": "Provides useful information but the reasoning could be more actionable..."
  }},
  "dimension_average": 3.4,
  "debate_adjustment": -0.5,
  "final_score": 2.9,
  "summary": "The response is acceptable but has issues with accuracy as identified by the Critic. While the Defender made some valid points, the overall quality is impacted by the misclassification of features."
}}
```

## IMPORTANT NOTES

- Judge each point FAIRLY - consider both arguments objectively
- The winner of each point should have the higher argument score
- `final_score` = `dimension_average` + `debate_adjustment`
- Debate adjustment: +0.5 (defender majority), -0.5 (critic majority), 0 (tie)
- Be specific in justifications - reference actual content
- Use the full 0-5 range for both argument scores and rubric scores
